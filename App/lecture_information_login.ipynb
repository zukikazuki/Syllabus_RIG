{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# セッションを開始\n",
    "session = requests.Session()\n",
    "\n",
    "# ログインページのURL\n",
    "login_url = \"https://syllabus.sfc.keio.ac.jp/users/sign_in?locale=ja&return_to=%2Fcourses%3Fbutton%3D%26locale%3Dja%26page%3D3%26search%255Bday_codes%255D%255B%255D%3D2%26search%255Bdepartments%255D%255B%255D%3D23%26search%255Bsemester%255D%3Dfall%26search%255Bsfc_guide_title%255D%3D%26search%255Bsub_semester%255D%3D%26search%255Bsummary%255D%3D%26search%255Bteacher_name%255D%3D%26search%255Btime_codes%255D%255B%255D%3D2%26search%255Btime_codes%255D%255B%255D%3D3%26search%255Btitle%255D%3D%26search%255Byear%255D%3D2024\"\n",
    "\n",
    "# ログインに必要なデータ (ユーザー名とパスワード)\n",
    "login_payload = {\n",
    "    'user[cns_account]': '',\n",
    "    'user[password]': ''\n",
    "}\n",
    "\n",
    "# ログインリクエストを送信\n",
    "response = session.post(login_url, data=login_payload)\n",
    "\n",
    "# ログイン成功したかどうかを確認\n",
    "if response.ok:\n",
    "    print(\"Login successful!\")\n",
    "else:\n",
    "    print(\"Login failed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 授業リンクを格納するリスト\n",
    "course_links_list = []\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# ページ番号を指定してスクレイピングを繰り返す\n",
    "for page in range(1, 28): \n",
    "    syllabus_url = f\"https://syllabus.sfc.keio.ac.jp/courses?button=&locale=ja&page={page}&search%5Bcommunication_type%5D%5B%5D=on_campus&search%5Bdepartments%5D%5B%5D=23&search%5Bobjective%5D=&search%5Bsemester%5D=fall&search%5Bsfc_guide_title%5D=&search%5Bsub_semester%5D=&search%5Bsummary%5D=&search%5Bteacher_name%5D=&search%5Btitle%5D=&search%5Byear%5D=2024\"\n",
    "    \n",
    "    syllabus_page = session.get(syllabus_url)\n",
    "    \n",
    "    soup = BeautifulSoup(syllabus_page.content, 'html.parser')\n",
    "    div_elements = soup.find_all('div', class_='detail-btn-wrapper')\n",
    "    for div in div_elements:\n",
    "        a_element = div.find('a', class_='detail-btn')\n",
    "        course_url = \"https://syllabus.sfc.keio.ac.jp\" + a_element['href']\n",
    "        course_links_list.append(course_url)\n",
    "        time.sleep(1)\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(course_links_list, columns=[\"Course Link\"])\n",
    "df.to_csv('course_links_taimen.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 対面授業以外\n",
    "# 授業リンクを格納するリスト\n",
    "course_links_list = []\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# ページ番号を指定してスクレイピングを繰り返す\n",
    "for page in range(1, 3):  # ページ数の範囲を適宜調整\n",
    "    syllabus_url = f\"https://syllabus.sfc.keio.ac.jp/courses?button=&locale=ja&page={page}&search%5Bcommunication_type%5D%5B%5D=online_live&search%5Bcommunication_type%5D%5B%5D=online_ondemand&search%5Bdepartments%5D%5B%5D=23&search%5Bobjective%5D=&search%5Bsemester%5D=fall&search%5Bsfc_guide_title%5D=&search%5Bsub_semester%5D=&search%5Bsummary%5D=&search%5Bteacher_name%5D=&search%5Btitle%5D=&search%5Byear%5D=2024\"\n",
    "    \n",
    "    # 各ページにリクエストを送信\n",
    "    syllabus_page = session.get(syllabus_url)\n",
    "    \n",
    "    # BeautifulSoupでHTMLをパース\n",
    "    soup = BeautifulSoup(syllabus_page.content, 'html.parser')\n",
    "    div_elements = soup.find_all('div', class_='detail-btn-wrapper')\n",
    "    for div in div_elements:\n",
    "        a_element = div.find('a', class_='detail-btn')\n",
    "        course_url = \"https://syllabus.sfc.keio.ac.jp\" + a_element['href']\n",
    "        course_links_list.append(course_url)\n",
    "        \n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(course_links_list, columns=[\"Course Link\"])\n",
    "df.to_csv('course_links_taimenigai.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_1 = pd.read_csv('course_links_taimen.csv')\n",
    "csv_2 = pd.read_csv('course_links_taimenigai.csv')\n",
    "csv_3 = pd.concat([csv_1, csv_2])\n",
    "csv_3.to_csv('course_links.csv', index=False, encoding='utf-8') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://syllabus.sfc.keio.ac.jp/courses/2024_45038?locale=ja'\n",
    "course_page = session.get(url)\n",
    "soup = BeautifulSoup(course_page.content, 'html.parser')\n",
    "soup_html = soup.prettify()\n",
    "with open('soup.html', 'w', encoding='utf-8') as file:\n",
    "    file.write(soup_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#改\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# DataFrameの初期化 \n",
    "df_course_info = pd.DataFrame(columns=[\n",
    "    '学部・研究科', '登録番号', '科目ソート', '科目名', '分野', '単位', '開講年度・学期', 'K-Number', \n",
    "    '同一科目', 'アスペクト', '研究会テーマ', '曜日・時限', '授業教員名', '実施形態', '授業で使う言語', '授業で英語サポート', \n",
    "    '履修条件', 'その他推奨知識', '開講場所', '授業形態', '履修制限', \n",
    "    '受け入れ予定人数', '課題提出タイプ', '学生が利用する予定機材/ソフト等', \n",
    "    '履修上の注意・留意事項', '参考文献', '連絡先メールアドレス', 'GIGAサティフィケート対象', \n",
    "    '講義概要', '主題と目標', '提出課題・試験・成績評価の方法など', '第1回', '第2回' , '第3回', '第4回', '第5回', '第6回', '第7回', '第8回', '第9回', '第10回', '第11回', '第12回', '第13回', '第14回', '第15回',\n",
    "    'その他'\n",
    "])\n",
    "\n",
    "def get_course_info_from_url(course_url):\n",
    "    # URLからページを取得\n",
    "    response = session.get(course_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    dynamic_fields = dict.fromkeys(df_course_info.columns, '')\n",
    "    \n",
    "    # 科目名の取得\n",
    "    h2 = soup.find('h2')\n",
    "    course_name_span = h2.find('span', class_='title')\n",
    "    if course_name_span:\n",
    "        course_name = course_name_span.get_text(strip=True)\n",
    "        dynamic_fields['科目名'] = course_name\n",
    "\n",
    "    # 各セクションを解析\n",
    "    contents_boxes = soup.find_all('div', class_='contents-box')\n",
    "    for box in contents_boxes:\n",
    "        h3 = box.find('h3')\n",
    "        if h3:\n",
    "            section_title = h3.get_text(strip=True)\n",
    "            if section_title == '授業概要':\n",
    "                parse_jugyo_gaiyo(box, dynamic_fields)\n",
    "            elif section_title == '詳細':\n",
    "                parse_shosai(box, dynamic_fields)\n",
    "            elif section_title == '授業計画':\n",
    "                parse_jugyo_keikaku(box, dynamic_fields)\n",
    "    \n",
    "    df_course_info.loc[len(df_course_info)] = [dynamic_fields[col] for col in df_course_info.columns]\n",
    "\n",
    "def parse_jugyo_gaiyo(box, dynamic_fields):\n",
    "    # '授業概要'セクション\n",
    "    # 'class-info'と'syllabus-info'を解析\n",
    "    class_info = box.find('div', class_='class-info')\n",
    "    if class_info:\n",
    "        subject = class_info.find('div', class_='subject')\n",
    "        if subject:\n",
    "            dl_rows = subject.find_all('dl', class_='row')\n",
    "            for dl in dl_rows:\n",
    "                dts = dl.find_all('dt')\n",
    "                dds = dl.find_all('dd')\n",
    "                for dt, dd in zip(dts, dds):\n",
    "                    dt_text = dt.get_text(strip=True)\n",
    "                    dd_text = dd.get_text(strip=True)\n",
    "                    # DataFrameのカラム名と一致するか確認\n",
    "                    colomns = df_course_info.drop(columns=['科目名'])\n",
    "                    if dt_text in colomns.columns:\n",
    "                        dynamic_fields[dt_text] = dd_text\n",
    "    syllabus_info = box.find('div', class_='syllabus-info')\n",
    "    if syllabus_info:\n",
    "        dl_rows = syllabus_info.find_all('dl', class_='row')\n",
    "        for dl in dl_rows:\n",
    "            dt = dl.find('dt')\n",
    "            dd = dl.find('dd')\n",
    "            if dt and dd:\n",
    "                dt_text = dt.get_text(strip=True)\n",
    "                dd_text = dd.get_text(strip=True)\n",
    "                if dt_text in colomns.columns:\n",
    "                    dynamic_fields[dt_text] = dd_text\n",
    "\n",
    "def parse_shosai(box, dynamic_fields):\n",
    "    # '詳細'セクション\n",
    "    detail_info = box.find('dl', class_='detail-info')\n",
    "    if detail_info:\n",
    "        dts = detail_info.find_all('dt')\n",
    "        dds = detail_info.find_all('dd')\n",
    "        for dt, dd in zip(dts, dds):\n",
    "            dt_text = dt.get_text(strip=True)\n",
    "            dd_text = dd.get_text(strip=True)\n",
    "            if dt_text in df_course_info.columns:\n",
    "                dynamic_fields[dt_text] = dd_text\n",
    "\n",
    "def parse_jugyo_keikaku(box, dynamic_fields):\n",
    "    # '授業計画'セクション\n",
    "    lecture_info = box.find('dl', class_='lecture-info')\n",
    "    if lecture_info:\n",
    "        dts = lecture_info.find_all('dt')\n",
    "        dds = lecture_info.find_all('dd')\n",
    "        for dt, dd in zip(dts, dds):\n",
    "            # 講義タイトルの取得\n",
    "            title_span = dt.find('span', class_='title')\n",
    "            if title_span:\n",
    "                lecture_title = title_span.get_text(strip=True)\n",
    "                # '第X回'を抽出\n",
    "                if lecture_title.startswith('第'):\n",
    "                    lecture_number = lecture_title.split(' ')[0]\n",
    "                    if lecture_number in df_course_info.columns:\n",
    "                        dd_text = dd.get_text(strip=True)\n",
    "                        dynamic_fields[lecture_number] = lecture_title + '\\n' + dd_text\n",
    "                elif lecture_title == 'その他':\n",
    "                    if 'その他' in df_course_info.columns:\n",
    "                        dd_text = dd.get_text(strip=True)\n",
    "                        dynamic_fields['その他'] = dd_text\n",
    "\n",
    "# 出力制限を変更\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# CSVファイルを読み込む\n",
    "course_links = pd.read_csv('course_links.csv')['Course Link']\n",
    "\n",
    "for course_link in course_links:  # 10件だけ取得\n",
    "    get_course_info_from_url(course_link)\n",
    "    \n",
    "df_course_info.to_csv('course_info.csv', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 出力制限を変更\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# CSVファイルを読み込む\n",
    "course_links = pd.read_csv('course_links.csv')['Course Link']\n",
    "\n",
    "# 試験的に10件の授業情報を取得\n",
    "for course_link in course_links[:10]:\n",
    "    get_course_info_from_url(course_link)\n",
    "    \n",
    "df_course_info.to_csv('course_info.csv', index=False, encoding='utf-8')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_course_html_from_url(course_url):\n",
    "    # URLからページを取得\n",
    "    response = session.get(course_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # dt と dd を取得してペアにする\n",
    "    dt_elements = soup.find_all('dt')\n",
    "    dd_elements = soup.find_all('dd')\n",
    "\n",
    "    n = 1\n",
    "    for dt, dd in zip(dt_elements, dd_elements):\n",
    "        dt_text = dt.get_text(strip=True)\n",
    "        dd_text = dd.get_text(strip=True)\n",
    "        print(f'dt{n}: {dt_text}')\n",
    "        print(f'dd{n}: {dd_text}')\n",
    "        n += 1\n",
    "\n",
    "get_course_html_from_url('https://syllabus.sfc.keio.ac.jp/courses/2024_39423?locale=ja')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://syllabus.sfc.keio.ac.jp/courses/2024_35765?locale=ja'\n",
    "\n",
    "df_course_info = pd.DataFrame(columns=[\n",
    "    '学部・研究科', '登録番号', '科目ソート', '科目名', '分野', '単位', '開講年度・学期', 'K-Number', \n",
    "    '同一科目', 'アスペクト', '研究会テーマ', '曜日・時限', '授業教員名', '実施形態', '授業で使う言語', '授業で英語サポート', \n",
    "    '履修条件', 'その他推奨知識', '開講場所', '授業形態', '履修制限', \n",
    "    '受け入れ予定人数', '課題提出タイプ', '学生が利用する予定機材/ソフト等', \n",
    "    '履修上の注意・留意事項', '参考文献', '連絡先メールアドレス', 'GIGAサティフィケート対象', \n",
    "    '講義概要', '主題と目標', '提出課題・試験・成績評価の方法など', '第1回', '第2回' , '第3回', '第4回', '第5回', '第6回', '第7回', '第8回', '第9回', '第10回', '第11回', '第12回', '第13回', '第14回', '第15回',\n",
    "    'その他'\n",
    "])\n",
    "\n",
    "def get_course_html_from_url_test(course_url):\n",
    "    # URLからページを取得\n",
    "    response = session.get(course_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    h2 = soup.find('h2')\n",
    "    if h2:\n",
    "        course_name_span = h2.find('span', class_='title')\n",
    "        if course_name_span:\n",
    "            course_name = course_name_span.get_text(strip=True)\n",
    "            print(f'科目名: {course_name}')\n",
    "\n",
    "get_course_info_from_url(url)\n",
    "df_course_info.to_csv('course_info_test.csv', index=False, encoding='utf-8')\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syllabus_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
